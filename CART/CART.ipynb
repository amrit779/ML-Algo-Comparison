{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Implementaion Using Scikit-Learn Library\n",
    "\n",
    "## Decision Tree\n",
    "Decision Trees are commonly used in data mining with the objective of creating a model that predicts the value of a target (or dependent variable) based on the values of several input (or independent variables). \n",
    "\n",
    "## CART\n",
    "The CART or Classification & Regression Trees methodology was introduced in 1984 by Leo Breiman, Jerome Friedman, Richard Olshen and Charles Stone as an umbrella term to refer to the following types of decision trees:\n",
    "\n",
    "### Classification Tree\n",
    "Where the target variable is categorical and the tree is used to identify the \"class\" within which a target variable would likely fall into.\n",
    "### Regression Trees\n",
    "Where the target variable is continuous and tree is used to predict it's value.\n",
    "\n",
    "The main elements of CART (and any decision tree algorithm) are:\n",
    "* Rules for splitting data at a node based on the value of one variable;\n",
    "* Stopping rules for deciding when a branch is terminal and can be split no more; and\n",
    "* Finally, a prediction for the target variable in each terminal node.\n",
    "\n",
    "### CART Model Representation\n",
    "The representation for the CART model is a binary tree.\n",
    "\n",
    "This is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable and a split point on that variable (assuming the variable is numeric).\n",
    "\n",
    "The leaf nodes of the tree contain an output variable which is used to make a prediction.\n",
    "\n",
    "#### Advantages\n",
    "* CART is nonparametric and therefore does not rely on data belonging to a particular type of distribution.\n",
    "* CART is not significantly impacted by outliers in the input variables.\n",
    "* You can relax stopping rules to \"overgrow\" decision trees and then prune back the tree to the optimal size.  This approach minimizes the probability that important structure in the data set will be overlooked by stopping too soon.\n",
    "* CART incorporates both testing with a test data set and cross-validation to assess the goodness of fit more accurately.\n",
    "* CART can use the same variables more than once in different parts of the tree.  This capability can uncover complex interdependencies between sets of variables.\n",
    "* CART can be used in conjunction with other prediction methods to select the input set of variables.\n",
    "\n",
    "#### Disadvantages\n",
    "* Overfitting\n",
    "* Decision tree can be unstable because of small change in data.\n",
    "* It cannot guarantee to return globally optimal decision tree.\n",
    "* Biased Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the modules required, from ScikitLearn Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amrit\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Inline Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_data(filename, filetype):\n",
    "    if filetype == 'csv':\n",
    "        dataset = pd.read_csv(filename)\n",
    "        nrow, ncol = dataset.shape\n",
    "        X = dataset.iloc[:, :ncol-1]\n",
    "        Y = dataset.iloc[:, ncol-1:ncol]\n",
    "        return X, Y\n",
    "    \n",
    "    elif filetype == 'libsvm':\n",
    "        mem = Memory(\"./mycache\")\n",
    "        @mem.cache\n",
    "        def get_data():\n",
    "            data = load_svmlight_file(filename)\n",
    "            return data[0], data[1]\n",
    "        X, Y = get_data()\n",
    "        return X, Y\n",
    "    \n",
    "    else:\n",
    "        print('File Type Not Supported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data \n",
    "\n",
    "#### Glass Identification Data Set \n",
    "From USA Forensic Science Service; 6 types of glass; defined in terms of their oxide content (i.e. Na, Fe, K, etc).\n",
    "Number of Instances : 214\n",
    "Number of Attributes : 10\n",
    "Attribute Information:\n",
    "\n",
    "1. Id number: 1 to 214 \n",
    "2. RI: refractive index \n",
    "3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10) \n",
    "4. Mg: Magnesium \n",
    "5. Al: Aluminum \n",
    "6. Si: Silicon \n",
    "7. K: Potassium \n",
    "8. Ca: Calcium \n",
    "9. Ba: Barium \n",
    "10. Fe: Iron \n",
    "11. Type of glass: (class attribute) \n",
    "    1. building_windows_float_processed \n",
    "    2. building_windows_non_float_processed \n",
    "    3. vehicle_windows_float_processed \n",
    "    4. vehicle_windows_non_float_processed (none in this database) \n",
    "    5. containers \n",
    "    6. tableware \n",
    "    7. headlamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__-E%3A-DM_Git-CART-__ipython-input__.get_data...\n",
      "get_data()\n",
      "_________________________________________________________get_data - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "# Dataset Splitting For Training and Testing\n",
    "X, Y = load_data('glass', 'libsvm')\n",
    "data = X\n",
    "target = Y\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0039980411529541016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amrit\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\tree\\_classes.py:319: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# fit a CART model to the data\n",
    "cfTree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "            max_features=None, max_leaf_nodes=10,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\n",
    "regTree = DecisionTreeRegressor(random_state=0)\n",
    "regTree.fit(data_train, target_train)\n",
    "\n",
    "startTime = time.time()\n",
    "cfTree.fit(data_train, target_train)\n",
    "endTime = time.time()\n",
    "timeDiff = endTime - startTime\n",
    "print(timeDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "expectedClass = target_test\n",
    "predictedClass = cfTree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.73      0.71        22\n",
      "         2.0       0.71      0.68      0.69        25\n",
      "         3.0       0.25      0.25      0.25         4\n",
      "         5.0       0.67      0.67      0.67         6\n",
      "         6.0       0.75      0.75      0.75         4\n",
      "         7.0       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.72        71\n",
      "   macro avg       0.68      0.68      0.68        71\n",
      "weighted avg       0.72      0.72      0.72        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expectedClass, predictedClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  4  2  0  0  0]\n",
      " [ 5 17  1  2  0  0]\n",
      " [ 2  1  1  0  0  0]\n",
      " [ 0  1  0  4  1  0]\n",
      " [ 0  1  0  0  3  0]\n",
      " [ 0  0  0  0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(expectedClass, predictedClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.83098591549296\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_metric(expectedClass, predictedClass)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the CF tree\n",
    "dot_data = export_graphviz(cfTree, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"DecisionTreeClassifier\")\n",
    "\n",
    "# Export the Reg tree\n",
    "dot_data = export_graphviz(regTree, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"DecisionTreeRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pruning\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Split Criterion</th>\n",
    "        <th>Max Depth</th>\n",
    "        <th>Max Leaf Node</th>\n",
    "        <th>Run time(in s)</th>\n",
    "        <th>Accuracy(%)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>10</td>\n",
    "        <td>10</td>\n",
    "        <td>0.0043</td>\n",
    "        <td>74.64</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>10</td>\n",
    "        <td>5</td>\n",
    "        <td>0.0040</td>\n",
    "        <td>70.42</td>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>5</td>\n",
    "        <td>10</td>\n",
    "        <td>0.0029</td>\n",
    "        <td>70.42</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>5</td>\n",
    "        <td>5</td>\n",
    "        <td>0.0033</td>\n",
    "        <td>71.83</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>5</td>\n",
    "        <td>10</td>\n",
    "        <td>0.0050</td>\n",
    "        <td>61.97</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>10</td>\n",
    "        <td>10</td>\n",
    "        <td>0.0054</td>\n",
    "        <td>61.97</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>5</td>\n",
    "        <td>5</td>\n",
    "        <td>0.0051</td>\n",
    "        <td>59.15</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>10</td>\n",
    "        <td>5</td>\n",
    "        <td>0.0061</td>\n",
    "        <td>59.15</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "* From the above we can deduce that Gini split is giving better accuracy than Entropy, So Gini Index can be the better option for the split criterion. And the runtime for Gini Split is also very less as compared to entropy.\n",
    "* In all the cases if we are increasing the depth of the tree the accuracy is increasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
