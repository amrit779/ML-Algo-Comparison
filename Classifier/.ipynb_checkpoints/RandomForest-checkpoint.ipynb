{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Tree Implementaion Using Scikit-Learn Library\n",
    "\n",
    "## Decision Tree\n",
    "Decision Trees are commonly used in data mining with the objective of creating a model that predicts the value of a target (or dependent variable) based on the values of several input (or independent variables). \n",
    "\n",
    "## Random Forests\n",
    "The general method of random decision forests was first proposed by Ho in 1995.\n",
    "\n",
    "### Classification Tree\n",
    "Where the target variable is categorical and the tree is used to identify the \"class\" within which a target variable would likely fall into.\n",
    "\n",
    "\n",
    "### Random Forest Representation\n",
    "The representation for the Random Forest model is collection of many Classification tree, that is why it is called forest.\n",
    "\n",
    "Random Forests grows many classification trees. To classify a new object from an input vector, put the input vector down each of the trees in the forest. Each tree gives a classification, and we say the tree \"votes\" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n",
    "\n",
    "#### Features\n",
    "* It is unexcelled in accuracy among current algorithms.\n",
    "* It runs efficiently on large data bases.\n",
    "* It can handle thousands of input variables without variable deletion.\n",
    "* It gives estimates of what variables are important in the classification.\n",
    "* It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
    "* It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "* It has methods for balancing error in class population unbalanced data sets.\n",
    "* Generated forests can be saved for future use on other data.\n",
    "* Prototypes are computed that give information about the relation between the variables and the classification.\n",
    "* It computes proximities between pairs of cases that can be used in clustering, locating outliers, or (by scaling) give interesting views of the data.\n",
    "* The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "* It offers an experimental method for detecting variable interactions.\n",
    "\n",
    "### The pseudocode for random forest algorithm can split into two stages.\n",
    "\n",
    "* Random forest creation pseudocode.\n",
    "* Pseudocode to perform prediction from the created random forest classifier.\n",
    "\n",
    "First, let’s begin with random forest creation pseudocode\n",
    "\n",
    "##### Random Forest pseudocode :\n",
    "1. Randomly select **“k”** features from total **“m”** features.\n",
    "    * Where **k << m**\n",
    "    \n",
    "2. Among the **“k”** features, calculate the node **“d”** using the best split point.\n",
    "\n",
    "3. Split the node into daughter nodes using the best split.\n",
    "\n",
    "4. Repeat 1 to 3 steps until **“l”** number of nodes has been reached.\n",
    "\n",
    "5. Build forest by repeating steps 1 to 4 for **“n”** number times to create **“n”** number of trees.\n",
    "\n",
    "Finally, we repeat 1 to 4 stages to create **“n”** randomly created trees. This randomly created trees forms the random forest.\n",
    "\n",
    "#### Random forest prediction pseudocode:\n",
    "\n",
    "To perform prediction using the trained random forest algorithm uses the below pseudocode.\n",
    "\n",
    "1. Takes the test features and use the rules of each randomly created decision tree to predict the oucome and stores the predicted outcome (target)\n",
    "\n",
    "2. Calculate the votes for each predicted target.\n",
    "\n",
    "3. Consider the high voted predicted target as the final prediction from the random forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the modules required, from ScikitLearn Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amrit\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Inline Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_data(filename, filetype):\n",
    "    if filetype == 'csv':\n",
    "        dataset = pd.read_csv(filename)\n",
    "        nrow, ncol = dataset.shape\n",
    "        X = dataset.iloc[:, :ncol-1]\n",
    "        Y = dataset.iloc[:, ncol-1:ncol]\n",
    "        return X, Y\n",
    "    \n",
    "    elif filetype == 'libsvm':\n",
    "        mem = Memory(\"./mycache\")\n",
    "        @mem.cache\n",
    "        def get_data():\n",
    "            data = load_svmlight_file(filename)\n",
    "            return data[0], data[1]\n",
    "        X, Y = get_data()\n",
    "        return X, Y\n",
    "    \n",
    "    else:\n",
    "        print('File Type Not Supported !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data \n",
    "\n",
    "#### Glass Identification Data Set \n",
    "From USA Forensic Science Service; 6 types of glass; defined in terms of their oxide content (i.e. Na, Fe, K, etc).\n",
    "Number of Instances : 214\n",
    "Number of Attributes : 10\n",
    "Attribute Information:\n",
    "\n",
    "1. Id number: 1 to 214 \n",
    "2. RI: refractive index \n",
    "3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10) \n",
    "4. Mg: Magnesium \n",
    "5. Al: Aluminum \n",
    "6. Si: Silicon \n",
    "7. K: Potassium \n",
    "8. Ca: Calcium \n",
    "9. Ba: Barium \n",
    "10. Fe: Iron \n",
    "11. Type of glass: (class attribute) \n",
    "    1. building_windows_float_processed \n",
    "    2. building_windows_non_float_processed \n",
    "    3. vehicle_windows_float_processed \n",
    "    4. vehicle_windows_non_float_processed (none in this database) \n",
    "    5. containers \n",
    "    6. tableware \n",
    "    7. headlamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__-E%3A-DM_Git-Classifier-__ipython-input__.get_data...\n",
      "get_data()\n",
      "_________________________________________________________get_data - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "# Dataset Splitting For Training and Testing\n",
    "X, Y = load_data('glass', 'libsvm')\n",
    "data = X\n",
    "target = Y\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1873645782470703\n"
     ]
    }
   ],
   "source": [
    "# fit a Random Forest model to the data\n",
    "cfTree = RandomForestClassifier(criterion='gini', max_depth=10, max_features='auto', max_leaf_nodes=10)\n",
    "\n",
    "startTime = time.time()\n",
    "cfTree.fit(data_train, target_train)\n",
    "endTime = time.time()\n",
    "timeDiff = endTime - startTime\n",
    "print(timeDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "expectedClass = target_test\n",
    "predictedClass = cfTree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(expectedClass, predictedClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metrics.confusion_matrix(expectedClass, predictedClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.64788732394366\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_metric(expectedClass, predictedClass)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pruning\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Split Criterion</th>\n",
    "        <th>Max Depth</th>\n",
    "        <th>Max Leaf Node</th>\n",
    "        <th>Run time(in s)</th>\n",
    "        <th>Accuracy(%)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>2</td>\n",
    "        <td>Default</td>\n",
    "        <td>0.022</td>\n",
    "        <td>60.56</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>10</td>\n",
    "        <td>5</td>\n",
    "        <td>0.025</td>\n",
    "        <td>70.42</td>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>5</td>\n",
    "        <td>10</td>\n",
    "        <td>0.036</td>\n",
    "        <td>73.23</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gini</td>\n",
    "        <td>5</td>\n",
    "        <td>5</td>\n",
    "        <td>0.039</td>\n",
    "        <td>70.42</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>5</td>\n",
    "        <td>10</td>\n",
    "        <td>0.039</td>\n",
    "        <td>69.01</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>10</td>\n",
    "        <td>10</td>\n",
    "        <td>0.043</td>\n",
    "        <td>69.01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>5</td>\n",
    "        <td>5</td>\n",
    "        <td>0.022</td>\n",
    "        <td>61.97</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Entropy</td>\n",
    "        <td>10</td>\n",
    "        <td>5</td>\n",
    "        <td>0.039</td>\n",
    "        <td>60.56</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "* From the above we can deduce that Gini split is giving better accuracy than Entropy, So Gini Index can be the better option for the split criterion. And the runtime for Gini Split is also very less as compared to entropy.\n",
    "* In all the cases if we are increasing the number of leaf nodes, the accuracy is increasing.\n",
    "* Random forest classifier handles the missing values.\n",
    "* When we have more trees in the forest, random forest classifier won’t overfit the model.\n",
    "* Can model the random forest classifier for categorical values also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
